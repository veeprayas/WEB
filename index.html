<!DOCTYPE html>
<html lang="en">

<head>
    <title>Yash Prakash</title>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css">
    <link rel="stylesheet" href="styles.css">
</head>

<body>

    <!-- Navigation Bar -->
    <nav class="navbar navbar-inverse navbar-fixed-top">
        <div class="container">
            <div class="navbar-header">
                <a class="navbar-brand" href="#">Yash Prakash</a>
            </div>
            <ul class="nav navbar-nav">
                <li><a href="#about">About Me</a></li>
                <li><a href="#publications">Publications<giot/a></li>
                <li><a href="#achievements">Achievements</a></li>
                <li><a href="#teaching">Teaching</a></li>
                <li><a href="#blogs">Blogs</a></li>

            </ul>
        </div>
    </nav>

    <!-- Hero Section -->
    <!DOCTYPE html>
    <html lang="en">

    <head>
        <title>Hero Section</title>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <link rel="stylesheet" href="styles.css">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css">
    </head>

    <body>
        <section class="hero">
            <div class="hero-content">
                <img src="promain1.png" alt="Profile Picture" class="profile-picture">
                <h1 class="name">Yash Prakash</h1>
                <h2 class="designation">PhD Candidate in Computer Science</h2>
                <div class="social-links">
                    <a href="https://www.linkedin.com/in/yash-prakash-030292162/" target="_blank" title="LinkedIn"><i
                            class="fa-brands fa-linkedin"></i></a>
                    <a href="https://github.com/accessodu" target="_blank" title="GitHub"><i
                            class="fa-brands fa-github"></i></a>
                    <a href="https://scholar.google.com/citations?hl=en&user=DOlm7Q8AAAAJ" target="_blank"
                        title="Google Scholar"><i class="fa-brands fa-google"></i></a>
                    <a href="https://x.com/LunaticBugbear" target="_blank" title="Twitter"><i
                            class="fa-brands fa-twitter"></i></a>
                    <a href="https://orcid.org/0000-0001-8593-327X" target="_blank" title="ORCID"><i
                            class="fa-brands fa-orcid"></i></a>
                </div>
                <div class="hero-details">
                    <p><strong>Research Interests:</strong> Human-Data Interaction, Human Centered AI, Infomation
                        Visualization,
                        Natural Language Processing</p>
                    <p><strong>Institution:</strong> Old Dominion University, Department of Computer Science</p>
                    <p><strong>Advisor:</strong> Dr. Vikas Gangigunte Ashok</p>
                </div>
            </div>
        </section>
    </body>

    </html>

    <!-- About Me Section -->
    <section id="about">
        <div class="container about-container">
            <h2 class="section-title">ABOUT ME</h2>
            <p>
                I am a PhD candidate in Computer Science at Old Dominion University. My research focuses on Human-Data
                Interaction, Human-Centered AI, Information Visualization, and Natural Language Processing. Under the
                guidance
                of Dr. Vikas Gangigunte Ashok, I aim to create solutions that enhance accessibility and usability of
                technology
                for diverse users.
            </p>
            <p>
                Beyond research, I enjoy mentoring students, contributing to open-source projects, and actively engaging
                in
                academic communities.
            </p>
        </div>
    </section>


    <!-- Publications Section -->
    <section id="publications" class="container">
        <h2 class="text-center">Publications</h2>

        <!-- Paper 1 -->
        <div class="paper-item row">
            <div class="col-md-5">
                <!-- Enlarged Image -->
                <img src="Picture1.png" class="teaser-image img-responsive" alt="Teaser Image"
                    style="margin-bottom: 20px;">
                <video class="teaser-video" controls width="100%">
                    <source src="Demo.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                </video>
            </div>
            <div class="col-md-7 paper-details">
                <div class="text-background">
                    <h4><a href="#">Towards Enhancing Low Vision Usability of Data Charts on Smartphones</a></h4>
                    <p><strong>Conference:</strong> IEEE VIS 2024 (Transactions on Visualization and Computer Graphics)
                    </p>
                    <p><strong>Authors:</strong> <strong>Yash Prakash</strong>, Pathan Aseef Khan, Akshay Kolgar Nayak,
                        Sampath
                        Jayarathna, Hae-Na
                        Lee, Vikas Ashok</p>
                    <p class="abstract">
                        The importance of data charts is self-evident, given their ability to express complex data in a
                        simple
                        format that facilitates quick and easy comparisons, analysis, and consumption. However, the
                        inherent visual
                        nature of the charts creates barriers for people with visual impairments to reap the associated
                        benefits to
                        the same extent as their sighted peers. While extant research has predominantly focused on
                        understanding and
                        addressing these barriers for blind screen reader users, the needs of low-vision screen
                        magnifier users have
                        been largely overlooked. In an interview study, almost all low-vision participants stated that
                        it was
                        challenging to interact with data charts on small screen devices such as smartphones and
                        tablets, even
                        though they could technically “see” the chart content. They ascribed these challenges mainly to
                        the
                        magnification-induced loss of visual context that connected data points with each other and also
                        with chart
                        annotations, e.g., axis values. In this paper, we present a method that addresses this problem
                        by
                        automatically transforming charts that are typically non-interactive images into personalizable
                        interactive
                        charts which allow selective viewing of desired data points and preserve visual context as much
                        as possible
                        under screen enlargement. We evaluated our method in a usability study with 26 low-vision
                        participants, who
                        all performed a set of representative chart-related tasks under different study conditions. In
                        the study, we
                        observed that our method significantly improved the usability of charts over both the status quo
                        screen
                        magnifier and a state-of-the-art space compaction-based solution.
                    </p>
                </div>
            </div>
        </div>

        <div class="paper-item row">
            <div class="col-md-5">
                <!-- Enlarged Image -->
                <img src="UserJourney (4).png" class="teaser-image img-responsive" style="width: 700px; height: 600px;"
                    alt="Teaser Image">
            </div>
            <div class="col-md-7 paper-details">
                <div class="text-background">
                    <h4><a href="https://doi.org/10.1145/3678957.3685714">Improving Usability of Data Charts in
                            Multimodal
                            Documents for Low Vision Users</a></h4>
                    <p><strong>Conference:</strong> ICMI '24: Proceedings of the 26th International Conference on
                        Multimodal
                        Interaction
                    </p>
                    <p><strong>Authors:</strong> Yash Prakash, Akshay Kolgar Nayak, Shoaib Mohammed Alyaan, Pathan Aseef
                        Khan,
                        Hae-Na Lee, Vikas Ashok</p>
                    <p class="abstract">
                        Data chart visualizations and text are often paired in news articles, online blogs, and academic
                        publications to present complex data. While chart visualizations offer graphical summaries of
                        the data, the
                        accompanying text provides essential context and explanation. Associating information from text
                        and charts
                        is straightforward for sighted users but presents significant challenges for individuals with
                        low vision,
                        especially on small-screen devices such as smartphones. The visual nature of charts coupled with
                        the layout
                        of the text inherently makes it difficult for low vision users to mentally associate chart data
                        with text
                        and comprehend the content due to their dependence on screen magnifier assistive technology,
                        which only
                        displays a small portion of the screen at any instant due to content enlargement. To address
                        this problem,
                        in this paper, we present a smartphone-based multimodal mixed-initiative interface that
                        transforms static
                        data charts and the accompanying text into an interactive slide show featuring frames containing
                        “magnified
                        views” of relevant data point combinations. The interface also includes a narration component
                        that delivers
                        tailored information for each “magnified view”. The design of our interface was informed by a
                        user study
                        with 10 low-vision participants, aimed at uncovering low vision interaction challenges and
                        user-interface
                        requirements with multimodal documents that integrate text and chart visualizations. Our
                        interface was also
                        evaluated in a subsequent study with 12 low-vision participants, where we observed significant
                        improvements
                        in chart usability compared to both status-quo screen magnifiers and a state-of-the-art
                        solution.
                    </p>
                </div>
            </div>
        </div>

        <div class="paper-item row">
            <div class="col-md-5">
                <!-- Enlarged Image -->
                <img src="Group UI (1).jpg" class="teaser-image img-responsive" alt="Teaser Image">
            </div>
            <div class="col-md-7 paper-details">
                <div class="text-background">
                    <h4><a href="https://doi.org/10.1145/3663548.3675616">Understanding Low Vision Graphical Perception
                            of Bar
                            Charts</a></h4>
                    <p><strong>Conference:</strong> ASSETS '24: Proceedings of the 26th International ACM SIGACCESS
                        Conference on
                        Computers and Accessibility
                    </p>
                    <p><strong>Authors:</strong> Yash Prakash, Akshay Kolgar Nayak, Sampath Jayarathna, Hae-Na Lee,
                        Vikas Ashok
                    </p>
                    <p class="abstract">
                        Bar charts are widely used for their simplicity in data representation, prompting numerous
                        studies to
                        explore and model how users interact with and perceive bar chart information. However, these
                        studies have
                        predominantly focused on sighted users, with a few also targeting blind screen-reader users,
                        whereas the
                        graphical perception of low-vision screen magnifier users is still an uncharted research
                        territory. We fill
                        this knowledge gap in this paper by designing four experiments for a laboratory study with 25
                        low-vision
                        participants to examine their graphical perception while interacting with bar charts. For our
                        investigation,
                        we built a custom screen magnifier-based logger that captured micro-interaction details such as
                        zooming and
                        panning. Our findings indicate that low-vision users invest significant time counteracting
                        blurring and
                        contrast effects when analyzing charts. We also observed that low-vision users struggle more in
                        interpreting
                        bars within a single-column stack compared to other stacked bar configurations, and moreover,
                        for a few
                        participants, the perception accuracy is lower when comparing separated bars than when comparing
                        adjacent
                        bars.
                    </p>
                </div>
            </div>
        </div>

        <div class="paper-item row">
            <div class="col-md-5">
                <!-- Enlarged Image -->
                <img src="Group 1 user (4).png" class="teaser-image img-responsive" alt="Teaser Image">
            </div>
            <div class="col-md-7 paper-details">
                <div class="text-background">
                    <h4><a href="https://doi.org/10.1145/3581641.3584049">

                            AutoDesc: Facilitating Convenient Perusal of Web Data Items for Blind Users</a></h4>
                    <p><strong>Conference:</strong> IUI '23: Proceedings of the 28th International Conference on
                        Intelligent User
                        Interfaces
                    </p>
                    <p><strong>Authors:</strong> Yash Prakash, Mohan Sunkara, Hae-Na Lee, Sampath Jayarathna, Vikas
                        Ashok
                    </p>
                    <p class="abstract">
                        Web data items such as shopping products, classifieds, and job listings are indispensable
                        components of most
                        e-commerce websites. The information on the data items are typically distributed over two or
                        more webpages,
                        e.g., a ‘Query-Results’ page showing the summaries of the items, and ‘Details’ pages containing
                        full
                        information about the items. While this organization of data mitigates information overload and
                        visual
                        cluttering for sighted users, it however increases the interaction overhead and effort for blind
                        users, as
                        back-and-forth navigation between webpages using screen reader assistive technology is tedious
                        and
                        cumbersome. Existing usability-enhancing solutions are unable to provide adequate support in
                        this regard as
                        they predominantly focus on enabling efficient content access within a single webpage, and as
                        such are not
                        tailored for content distributed across multiple webpages. As an initial step towards addressing
                        this issue,
                        we developed AutoDesc, a browser extension that leverages a custom extraction model to
                        automatically detect
                        and pull out additional item descriptions from the ‘details’ pages, and then proactively inject
                        the
                        extracted information into the ‘Query-Results’ page, thereby reducing the amount of
                        back-and-forth screen
                        reader navigation between the two webpages. In a study with 16 blind users, we observed that
                        within the same
                        time duration, the participants were able to peruse significantly more data items on average
                        with AutoDesc,
                        compared to that with their preferred screen readers as well as with a state-of-the-art
                        solution.
                    </p>
                </div>
            </div>
        </div>

        <div class="paper-item row">
            <div class="col-md-5">
                <!-- Enlarged Image -->
                <img src="Teaser (7).png" class="teaser-image img-responsive" style="margin-bottom: 20px;"
                    alt="Teaser Image">
                <img src="Architecture (13).png" class="teaser-image img-responsive" alt="Teaser Image">
            </div>
            <div class="col-md-7 paper-details">
                <div class="text-background">
                    <h4><a href="https://doi.org/10.1145/3664639">

                            All in One Place: Ensuring Usable Access to Online Shopping Items for Blind Users</a></h4>
                    <p><strong>Conference:</strong> Proceedings of the ACM on Human-Computer Interaction, Volume 8,
                        Issue EICS
                    </p>
                    <p><strong>Authors:</strong> Yash Prakash, Akshay Kolgar Nayak, Mohan Sunkara, Sampath Jayarathna,
                        Hae-Na Lee,
                        Vikas Ashok
                    </p>
                    <p class="abstract">
                        Perusing web data items such as shopping products is a core online user activity. To prevent
                        information
                        overload, the content associated with data items is typically dispersed across multiple webpage
                        sections
                        over multiple web pages. However, such content distribution manifests an unintended side effect
                        of
                        significantly increasing the interaction burden for blind users, since navigating to-and-fro
                        between
                        different sections in different pages is tedious and cumbersome with their screen readers. While
                        existing
                        works have proposed methods for the context of a single webpage, solutions enabling usable
                        access to content
                        distributed across multiple webpages are few and far between. In this paper, we present
                        InstaFetch, a
                        browser extension that dynamically generates an alternative screen reader-friendly user
                        interface in
                        real-time, which blind users can leverage to almost instantly access different item-related
                        information such
                        as description, full specification, and user reviews, all in one place, without having to
                        tediously navigate
                        to different sections in different webpages. Moreover, InstaFetch also supports natural language
                        queries
                        about any item, a feature blind users can exploit to quickly obtain desired information, thereby
                        avoiding
                        manually trudging through reams of text. In a study with 14 blind users, we observed that the
                        participants
                        needed significantly lesser time to peruse data items with InstaFetch, than with a
                        state-of-the-art
                        solution.
                    </p>
                </div>
            </div>
        </div>

        <div class="paper-item row">
            <div class="col-md-5">
                <!-- Enlarged Image -->
                <img src="cloud_access.png" class="teaser-image img-responsive" alt="Teaser Image">

            </div>
            <div class="col-md-7 paper-details">
                <div class="text-background">
                    <h4><a href="https://doi.org/10.1145/3648188.367515">

                            You Shall Know a Forum by the Words they Keep: Analyzing Language Use in Accessibility
                            Forums for Blind
                            Users</a></h4>
                    <p><strong>Conference:</strong> HT '24: Proceedings of the 35th ACM Conference on Hypertext and
                        Social Media
                    </p>
                    <p><strong>Authors:</strong> Nithiya Venkatraman, Anand Aiyer, Yash Prakash, Vikas Ashok
                    </p>
                    <p class="abstract">
                        Discussion forums are one of the favored platforms for knowledge sharing. Given their
                        popularity, copious
                        research exists on understanding linguistic and behavioral characteristics of forum
                        conversations. However,
                        prior investigations have mainly focused on general forums designed primarily for sighted users,
                        and as such
                        the applicability of their findings to the dedicated accessibility discussion forums frequented
                        by blind
                        individuals remains unanswered. To bridge this knowledge gap, and facilitate the development of
                        better-informed assistive technologies for blind people, we investigated language use and
                        identified the key
                        semantic and cognitive characteristics of online accessibility forums. To aid our investigation,
                        we
                        collected a dataset of 1000 accessibility forum threads and a baseline of 1000 general forum
                        threads. These
                        threads were carefully curated to ensure similarity of topics discussed. We found the language
                        in
                        accessibility forum conversations to be more task-oriented and less abstract, with significantly
                        higher
                        number of descriptive action words than in general forum conversations. Results also showed an
                        emphasis on
                        sharing first-hand personal experiences in accessibility forums, relative to the general forums.
                    </p>
                </div>
            </div>
        </div>

        <div class="paper-item row">
            <div class="col-md-5">
                <!-- Enlarged Image -->

                <img src="workflow.png" class="teaser-image img-responsive" alt="Teaser Image"
                    style="margin-bottom: 20px;">
                <img src="teaser (8).png" class="teaser-image img-responsive" alt="Teaser Image">

            </div>
            <div class="col-md-7 paper-details">
                <div class="text-background">
                    <h4><a href="https://doi.org/10.1145/3593228">

                            Enabling Customization of Discussion Forums for Blind Users</a></h4>
                    <p><strong>Conference:</strong> Proceedings of the ACM on Human-Computer Interaction, Volume 7,
                        Issue EICS
                    </p>
                    <p><strong>Authors:</strong> Mohan Sunkara, Yash Prakash, Hae-Na Lee, Sampath Jayarathna, Vikas
                        Ashok
                    </p>
                    <p class="abstract">
                        Online discussion forums have become an integral component of news, entertainment, information,
                        and
                        video-streaming websites, where people all over the world actively engage in discussions on a
                        wide range of
                        topics including politics, sports, music, business, health, and world affairs. Yet, little is
                        known about
                        their usability for blind users, who aurally interact with the forum conversations using screen
                        reader
                        assistive technology. In an interview study, blind users stated that they often had an arduous
                        and
                        frustrating interaction experience while consuming conversation threads, mainly due to the
                        highly redundant
                        content and the absence of customization options to selectively view portions of the
                        conversations. As an
                        initial step towards addressing these usability concerns, we designed PView - a browser
                        extension that
                        enables blind users to customize the content of forum threads in real time as they interact with
                        these
                        threads. Specifically, PView allows the blind users to explicitly hide any post that is
                        irrelevant to them,
                        and then PView automatically detects and filters out all subsequent posts that are substantially
                        similar to
                        the hidden post in real time, before the users navigate to those portions of the thread. In a
                        user study
                        with blind participants, we observed that compared to the status quo, PView significantly
                        improved the
                        usability, workload, and satisfaction of the participants while interacting with the forums.
                    </p>
                </div>
            </div>
        </div>

        <div class="paper-item row">
            <div class="col-md-5">
                <!-- Enlarged Image -->

                <img src="image1.png" class="teaser-image img-responsive" alt="Teaser Image"
                    style="margin-bottom: 20px;">
                <img src="image2.png" class="teaser-image img-responsive" alt="Teaser Image">

            </div>
            <div class="col-md-7 paper-details">
                <div class="text-background">
                    <h4><a href="https://doi.org/10.1145/3511095.353127">

                            Enabling Customization of Discussion Forums for Blind Users</a></h4>
                    <p><strong>Conference:</strong> HT '22: Proceedings of the 33rd ACM Conference on Hypertext and
                        Social Media
                    </p>
                    <p><strong>Authors:</strong> Hae-Na Lee, Yash Prakash, Mohan Sunkara, IV Ramakrishnan, Vikas Ashok
                    </p>
                    <p class="abstract">
                        Online collaborative editors have become increasingly prevalent in both professional and
                        academic settings.
                        However, little is known about how usable these editors are for low vision screen magnifier
                        users, as
                        existing research works have predominantly focused on blind screen reader users. An interview
                        study revealed
                        that it is arduous and frustrating for screen magnifier users to perform even the basic
                        collaborative
                        writing activities, such as addressing collaborators’ comments and reviewing document changes.
                        Specific
                        interaction challenges underlying these issues included excessive panning, content occlusion,
                        large empty
                        space patches, and frequent loss of context. To address these challenges, we developed MagDocs,
                        a browser
                        extension that assists screen magnifier users in conveniently performing collaborative writing
                        activities on
                        the Google Docs web application. MagDocs is rooted in two ideas: (i) a custom support interface
                        that users
                        can instantly access on demand and interact with collaborative interface elements, such as
                        comments or
                        collaborator edits, within the current magnifier viewport; and (ii) visual relationship
                        preservation, where
                        collaborative elements and the corresponding text in the document are shown close to each other
                        within the
                        magnifier viewport to minimize context loss and panning effort. A study with 15 low vision users
                        showed that
                        MagDocs significantly improved the overall user satisfaction and interaction experience, while
                        also
                        substantially reduced the time and effort to perform typical collaborative writing tasks.
                    </p>
                </div>
            </div>
        </div>

        <!-- Add more papers here in the same format -->
    </section>



    <!-- Achievements Section -->
    <section id="achievements" class="container">
        <h2 class="text-center">Achievements</h2>
        <ul class="achievements-list">
            <li><strong>Dominion Graduate Scholar:</strong> Awarded Competitive Graduate Scholar Distinction</li>
            <li><strong>Top Reviewer:</strong> Made it to the Top Reviewer list of NeurIPS 2023 as Ethics Reviewer</li>
            <li><strong>Outstanding Reviews:</strong> Special Recognition for Outstanding Reviews for CUI 2024 Papers
            </li>
            <li><strong>ODU Summer Internship:</strong> Won Best Budding Data Scientist Award</li>
            <li><strong>Start-up Challenge Finalist:</strong> Pravega, Indian Institute of Science, Bangalore</li>
            <li><strong>National Student Start-up Challenge Finalist:</strong> Confederation of Indian Industry Start-up
                Challenge</li>
            <li><strong>Coding Challenge Winner:</strong> Won PRA Group Summer Coding Challenge</li>
            <li><strong>Hackathon Finalist:</strong> Eureka Smart India Hackathon</li>
            <li><strong>National Level Declamation:</strong> (A.S.I.S.C) National Level Declamation</li>
            <li><strong>State Level Declamation:</strong> (K.I.S.A) State Level Declamation - Gold Medal</li>
        </ul>
    </section>

    <section id="engagements" class="engagements-section">
        <div class="container">
            <h2 class="section-title">Reviewing History</h2>
            <ul class="engagement-list">
                <li>CHI 2025 Case Studies</li>
                <li>IUI 2025 Papers</li>
                <li>CHI 2024 Case Studies</li>
                <li>CUI 2024 Papers</li>
                <li>ICMI 2024 Long and Short Papers</li>
                <li>IDC 2024 Work-In-Progress</li>
                <li>IMX 2024 Technical Papers</li>
                <li>IUI 2024 Posters and Demos</li>
                <li>UbiComp/ISWC 2024 Posters and Demos</li>
                <li>NeurIPS Ethics Reviewer 2024</li>
                <li>IUI 2023 Posters and Demos</li>
                <li>NeurIPS Ethics Reviewer 2023</li>
            </ul>
        </div>
    </section>

    <!DOCTYPE html>
    <html lang="en">

    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Teaching Experience</title>
        <link rel="stylesheet" href="styles.css">
    </head>

    <body>
        <section id="teaching" class="teaching-section">
            <h2>Teaching Experience</h2>

            <!-- Course Item 1 -->
            <div class="course-item">
                <div class="course-header">
                    <h4>CS 361: Data Structures and Algorithms</h4>
                    <p>Instructor: Dr. Morris</p>
                </div>
                <div class="course-details">
                    <h5>Course Details</h5>
                    <p><strong>Title:</strong> Data Structures and Algorithms</p>
                    <p><strong>Credit Hours:</strong> 3</p>
                    <p><strong>Start Date:</strong> 08/24/2024</p>
                    <p><strong>End Date:</strong> 12/06/2024</p>
                    <p><strong>Description:</strong> This course covers the following topics: (i) Basic introduction to
                        algorithms, their design and analysis (ii) Asymptotic notation (iii) Recurrence Relations and
                        their
                        solutions (iv) Sorting and Order Statistics: various algorithms for sorting and their analysis,
                        lower bounds
                        for sorting, computing medians, modes and various order statistics (v) Paradigms for algorithm
                        design and
                        analysis: Dynamic Programming, Greedy Method, Amortized Analysis, and (vi) Graphs and Elementary
                        Graph
                        Algorithms: Breadth-first and Depth-first Search, Topological Sort, Minimum Spanning Trees and
                        Shortest
                        Paths Algorithms</p>
                </div>
            </div>

            <!-- Course Item 2 -->
            <div class="course-item">
                <div class="course-header">
                    <h4>CS 722/822: Machine Learning</h4>
                    <p>Instructor: Dr. Sun</p>
                </div>
                <div class="course-details">
                    <h5>Course Details</h5>
                    <p><strong>Title:</strong> Machine Learning</p>
                    <p><strong>Credit Hours:</strong> 3</p>
                    <p><strong>Start Date:</strong> 08/24/2024</p>
                    <p><strong>End Date:</strong> 12/06/2024</p>
                    <p><strong>Description:</strong> This course presents both the foundational and the practical
                        aspects of
                        modeling, analyzing, and mining of computerized data sets, including classification, regression,
                        clustering,
                        semi-supervised learning, structured sparsity learning, etc. The course assignments are designed
                        to contain
                        both theoretical and programming components in order to train students to gain
                        hands-on-experience.</p>
                </div>
            </div>

            <div class="course-item">
                <div class="course-header">
                    <h4>CS 480/580: Introduction to AI</h4>
                    <p>Instructor: Dr. Yaohang Li</p>
                </div>
                <div class="course-details">
                    <h5>Course Details</h5>
                    <p><strong>Title:</strong> Introduction to AI</p>
                    <p><strong>Credit Hours:</strong> 3</p>
                    <p><strong>Start Date:</strong> 08/24/2024</p>
                    <p><strong>End Date:</strong> 12/06/2024</p>
                    <p><strong>Description:</strong> Laboratory work required. Introduction to concepts, principles,
                        challenges,
                        and research in major areas of AI. Areas of discussion include: natural language and vision
                        processing,
                        machine learning, machine logic and reasoning, robotics, expert and mundane systems.</p>
                </div>
            </div>

            <div class="course-item">
                <div class="course-header">
                    <h4>CS 733/833: Natural Language Processing</h4>
                    <p>Instructor: Dr. Vikas</p>
                </div>
                <div class="course-details">
                    <h5>Course Details</h5>
                    <p><strong>Title:</strong> Natural Language Processing</p>
                    <p><strong>Credit Hours:</strong> 3</p>
                    <p><strong>Start Date:</strong> 08/24/2024</p>
                    <p><strong>End Date:</strong> 12/06/2024</p>
                    <p><strong>Description:</strong> Natural language processing (NLP) techniques are the crux of many
                        leading
                        modern technologies. Advances in NLP are also critical in the pursuit of Artificial
                        Intelligence. This
                        course will discuss core problems in NLP and the state-of-the-art tools and techniques as well
                        as advanced
                        NLP research topics. The topics will include language models, part-of-speech tagging, syntactic
                        parsing,
                        word embedding, statistical machine translation, text summarization, question answering, and
                        dialog
                        interaction. At the end of the course, students will be familiar with many language-processing
                        tasks and
                        applications.</p>
                </div>
            </div>
        </section>

        <script>
            // JavaScript to toggle visibility of course details
            document.querySelectorAll('.course-header').forEach(header => {
                header.addEventListener('click', () => {
                    const details = header.nextElementSibling;
                    details.style.display = (details.style.display === 'block') ? 'none' : 'block';
                });
            });
        </script>
    </body>

    </html>

    <!-- Blogs Section -->
    <section id="blogs" class="container">
        <h2 class="text-center">Latest Blogs</h2>

        <!-- Blog 1 -->
        <div class="blog-item">
            <h4>
                <a href="https://ws-dl.blogspot.com/2024/12/2024-12-06-ieee-vis-2024-trip-report.html" target="_blank"
                    title="Dynamic Screen Reader Enhancements">
                    2024-12-06: IEEE VIS 2024 Trip Report
                </a>
            </h4>
            <p>Published • Dec 6</p>
            <div class="blog-keywords">
                <span>IEEE VIS 2024</span>
                <span>Trip Report</span>
                <span>Virtual Conference</span>
                <span>TVCG</span>
            </div>
        </div>

        <!-- Blog 2 -->
        <div class="blog-item">
            <h4>
                <a href="https://ws-dl.blogspot.com/2024/11/2024-11-27-my-experience-with-ocr-post.html" target="_blank"
                    title="Multi-Agent Systems for Accessibility">
                    2024-11-27: My Experience with OCR Post-Correction
                </a>
            </h4>
            <p>Published • Nov 27</p>
            <div class="blog-keywords">
                <span>Spelling Auto-correction</span>
                <span>OCR Technology</span>
                <span>OCR Posr-Correction</span>
            </div>
        </div>

        <!-- Blog 3 -->
        <div class="blog-item">
            <h4>
                <a href="https://ws-dl.blogspot.com/2024/08/2024-08-22-paper-summary-all-in-one.html" target="_blank"
                    title="Usability Challenges for Low Vision Users">
                    2024-08-22: Paper Summary: "All in One Place: Ensuring Usable Access to Online Shopping Items for
                    Blind Users"
                </a>
            </h4>
            <p>Published • Aug 22</p>
            <div class="blog-keywords">
                <span>Paper Summary</span>
                <span>PACMHCI</span>
                <span>Online Shopping</span>
                <span>Visual Impairment</span>
            </div>
        </div>

        <div class="blog-item">
            <h4>
                <a href="https://ws-dl.blogspot.com/2023/12/2023-12-29-paper-summary-modeling-touch.html"
                    target="_blank" title="Usability Challenges for Low Vision Users">
                    2023-12-29: Paper Summary: "Modeling Touch-based Menu Selection Performance of Blind Users via
                    Reinforcement
                    Learning"
                </a>
            </h4>
            <p>Published • Dec 29</p>
            <div class="blog-keywords">
                <span>Paper Summary</span>
                <span>CHI</span>
                <span>Screen Reader</span>
                <span>Menu Accessiblity</span>
            </div>
        </div>

        <div class="blog-item">
            <h4>
                <a href="https://ws-dl.blogspot.com/2023/12/2023-12-28-paper-summary-enabling.html" target="_blank"
                    title="Usability Challenges for Low Vision Users">
                    2023-12-29: Paper Summary: "Enabling Customization of Discussion Forums for Blind Users"
                </a>
            </h4>
            <p>Published • Dec 29</p>
            <div class="blog-keywords">
                <span>Paper Summary</span>
                <span>PACMHCI</span>
                <span>Discussion Forums</span>
                <span>Web Accessibility</span>
            </div>
        </div>

        <div class="blog-item">
            <h4>
                <a href="https://ws-dl.blogspot.com/2023/12/2023-12-29-paper-summary-autodesc.html" target="_blank"
                    title="Usability Challenges for Low Vision Users">
                    2023-12-29: Paper Summary: "AutoDesc: Facilitating Convenient Perusal of Web Data Items for Blind
                    Users"
                </a>
            </h4>
            <p>Published • Dec 29</p>
            <div class="blog-keywords">
                <span>Paper Summary</span>
                <span>IUI</span>
                <span>Screen Reader</span>
                <span>Web Accessibility</span>
            </div>
        </div>

        <div class="blog-item">
            <h4>
                <a href="https://ws-dl.blogspot.com/2022/07/2022-07-24-acm-conference-on-hypertext.html" target="_blank"
                    title="Usability Challenges for Low Vision Users">
                    2022-07-24: ACM Conference on Hypertext and Social Media Trip Report
                </a>
            </h4>
            <p>Published • Dec 29</p>
            <div class="blog-keywords">
                <span>Trip Report</span>
                <span>Hypertext</span>
                <span>Virtual COnference</span>
            </div>
        </div>

    </section>

    <!-- Twitter Widget -->
    <section id="twitter" class="container text-center">
        <h2>My Latest Tweets</h2>
        <a class="twitter-timeline" href="https://x.com/LunaticBugbear" data-width="600" data-height="500"
            data-theme="dark">
            Tweets by @LunaticBugbear
        </a>
        <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
    </section>


    <!-- Footer -->
    <footer class="text-center" style="margin-top: 20px; padding: 10px; background: #333; color: white;">
        <p>&copy; 2024 Yash Prakash | All Rights Reserved</p>
    </footer>

    <!-- Scripts -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.12.0/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/js/bootstrap.min.js"></script>
</body>

</html>
